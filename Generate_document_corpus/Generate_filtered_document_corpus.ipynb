{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9578344b-4b39-45e7-a11d-b54738b48346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1a61f-34c1-4d6a-a2a6-f1cff91702dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_dataset(\"igzi/pile-stem-corpus\", split=\"train\")\n",
    "queries = load_dataset(\"igzi/MNLP_M2_mcqa_dataset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "409f9f34-632e-49a1-9e49-1c23be9cd577",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_chunks = [doc[\"text\"] for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7928f09-5db1-40d2-9fd1-60602cd88581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract texts\n",
    "mmlu_queries = []\n",
    "arc_queries = []\n",
    "mathqa_queries =[]\n",
    "scienceqa_queries = []\n",
    "\n",
    "for query in queries:\n",
    "    if query[\"dataset\"] == \"kz919/mmlu-auxiliary-train-auto-labelled\":\n",
    "        mmlu_queries.append(query[\"question\"])\n",
    "    elif query[\"dataset\"] == \"allenai/ai2_arc\" and \"arc_easy_\" in query[\"id\"]:\n",
    "        arc_queries.append(query[\"question\"])\n",
    "    elif query[\"dataset\"] == \"derek-thomas/ScienceQA\":\n",
    "        scienceqa_queries.append(query[\"question\"])\n",
    "    elif query[\"dataset\"] == \"allenai/math_qa\":\n",
    "        mathqa_queries.append(query[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5452cb4a-2374-4cce-989e-fd33641d0225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combined queries: 900\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(mmlu_queries)\n",
    "random.shuffle(arc_queries)\n",
    "random.shuffle(scienceqa_queries)\n",
    "random.shuffle(mathqa_queries)\n",
    "\n",
    "# Sample specified number of queries from each\n",
    "final_queries = (\n",
    "    mmlu_queries[:300] +\n",
    "    arc_queries[:300] +\n",
    "    scienceqa_queries[:150] +\n",
    "    mathqa_queries[:150]\n",
    ")\n",
    "\n",
    "print(f\"Total combined queries: {len(final_queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82cd80c1-c030-46a4-9cf2-2031fb1ae9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e56b3b96-78b1-4db0-a032-7397e217cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_filter(chunks, tokenizer, min_tokens=50, max_tokens=400):\n",
    "    def is_valid(chunk):\n",
    "        tokenized = tokenizer(chunk, truncation=False, add_special_tokens=False)\n",
    "        length = len(tokenized[\"input_ids\"])\n",
    "        if not (min_tokens <= length <= max_tokens):\n",
    "            return False\n",
    "        if len(set(chunk.split())) / len(chunk.split()) < 0.5:  # still use for redundancy\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    return [chunk for chunk in chunks if is_valid(chunk)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c796b921-8fb8-489e-a5b5-2e0a672b2e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "filtered_chunks = heuristic_filter(corpus_chunks, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d9a061-840b-4813-a79d-d85d8e80b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_chunks_set = set(filtered_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac6fc194-b925-4db2-a6f8-6e228d94bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_corpus = []\n",
    "for chunk in corpus:\n",
    "    if chunk[\"text\"] in filtered_chunks_set:\n",
    "        filtered_corpus.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a9cfb-64ee-4e52-96ce-e79ff7780e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# Step 1: Create a Hugging Face Dataset object\n",
    "filtered_dataset = Dataset.from_list(filtered_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6731945-6262-4fd6-8092-035a195949b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Save locally\n",
    "filtered_dataset.save_to_disk(\"pile-stem-corpus-filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c36641-a14a-4ed3-b13c-160e8d7f7f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: (Optional) Push to Hugging Face Hub\n",
    "filtered_dataset.push_to_hub(\"igzi/pile-stem-corpus-filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79da4dde-50f3-436a-9e85-9c2ff965cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce8ac90-fc20-4184-8987-9d4916afcddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = load_dataset(\"igzi/pile-stem-corpus-filtered\", split=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNP Project",
   "language": "python",
   "name": "mnlp_m2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
