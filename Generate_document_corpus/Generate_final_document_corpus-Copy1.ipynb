{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da9ab190-e351-4c34-9d93-c86c7ab2ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa5b97cd-a792-4fcd-b307-73249f372e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_dataset(\"igzi/pile-stem-corpus-filtered-embedded\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b6f54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 250000\n",
    "subset = corpus.select(range(subset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed13fb43-bc04-46f0-89df-73f7fcc3413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_wiki = []\n",
    "text_wiki = []\n",
    "embeddings_stackexchange = []\n",
    "text_stackexchange = []\n",
    "embeddings_textbook = []\n",
    "text_textbook = []\n",
    "\n",
    "for chunk in subset:\n",
    "    if chunk[\"source\"] == \"https://huggingface.co/datasets/milkshake721/2.1M-wiki-STEM\":\n",
    "        embeddings_wiki.append(chunk[\"embedding\"])\n",
    "        text_wiki.append(chunk[\"text\"])\n",
    "    elif chunk[\"source\"] == \"https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_math_jsonl\":\n",
    "        embeddings_stackexchange.append(chunk[\"embedding\"])\n",
    "        text_stackexchange.append(chunk[\"text\"])\n",
    "    else:\n",
    "        embeddings_textbook.append(chunk[\"embedding\"])\n",
    "        text_textbook.append(chunk[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db352b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "embeddings_wiki_norm = normalize(np.array(embeddings_wiki))\n",
    "embeddings_stackexchange_norm = normalize(np.array(embeddings_stackexchange))\n",
    "embeddings_textbook_norm = normalize(np.array(embeddings_textbook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f473c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki: 190534\n",
      "Stackexchange: 49001\n",
      "Textbook: 10465\n"
     ]
    }
   ],
   "source": [
    "print(f\"Wiki: {len(embeddings_wiki)}\")\n",
    "print(f\"Stackexchange: {len(embeddings_stackexchange)}\")\n",
    "print(f\"Textbook: {len(embeddings_textbook)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "num_clusters_wiki = 750\n",
    "kmeans = KMeans(n_clusters=num_clusters_wiki, random_state=42, n_init=\"auto\")\n",
    "cluster_labels_wiki = kmeans.fit_predict(embeddings_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de4aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters_stackexchange = 200\n",
    "kmeans = KMeans(n_clusters=num_clusters_stackexchange, random_state=42, n_init=\"auto\")\n",
    "cluster_labels_stackexchange = kmeans.fit_predict(embeddings_stackexchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2317a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters_textbook = 50\n",
    "kmeans = KMeans(n_clusters=num_clusters_textbook, random_state=42, n_init=\"auto\")\n",
    "cluster_labels_textbook = kmeans.fit_predict(embeddings_textbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36fc8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEPARATOR = \"\\n#{1,6} \"\n",
    "\n",
    "merged_wiki_docs = []\n",
    "for cluster_id in range(num_clusters_wiki):\n",
    "    # Get indices of texts in this cluster\n",
    "    indices = np.where(cluster_labels_wiki == cluster_id)[0]\n",
    "    # Combine texts with the separator\n",
    "    merged_text = SEPARATOR + SEPARATOR.join([text_wiki[i] for i in indices])\n",
    "    merged_wiki_docs.append(merged_text)\n",
    "\n",
    "# Repeat for StackExchange\n",
    "merged_stackexchange_docs = []\n",
    "for cluster_id in range(num_clusters_stackexchange):\n",
    "    indices = np.where(cluster_labels_stackexchange == cluster_id)[0]\n",
    "    merged_text = SEPARATOR + SEPARATOR.join([text_stackexchange[i] for i in indices])\n",
    "    merged_stackexchange_docs.append(merged_text)\n",
    "\n",
    "# Repeat for Textbook\n",
    "merged_textbook_docs = []\n",
    "for cluster_id in range(num_clusters_textbook):\n",
    "    indices = np.where(cluster_labels_textbook == cluster_id)[0]\n",
    "    merged_text = SEPARATOR + SEPARATOR.join([text_textbook[i] for i in indices])\n",
    "    merged_textbook_docs.append(merged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c979a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09d682fd82e4ca78850b24b858adc26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e35ea75b014c7da29a5980031433fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4103fcd7ee4cb1a37ac2ce363e94d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/311 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/igzi/pile-stem-corpus-small-semantic/commit/12c7f41eaacbd7592c85264781a203efc83dc475', commit_message='Upload dataset', commit_description='', oid='12c7f41eaacbd7592c85264781a203efc83dc475', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/igzi/pile-stem-corpus-small-semantic', endpoint='https://huggingface.co', repo_type='dataset', repo_id='igzi/pile-stem-corpus-small-semantic'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Prepare merged docs and sources\n",
    "all_merged_docs = (\n",
    "    merged_wiki_docs + merged_stackexchange_docs + merged_textbook_docs\n",
    ")\n",
    "all_sources = (\n",
    "    [\"https://huggingface.co/datasets/milkshake721/2.1M-wiki-STEM\"] * len(merged_wiki_docs)\n",
    "    + [\"https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_math_jsonl\"] * len(merged_stackexchange_docs)\n",
    "    + [\"https://huggingface.co/datasets/izumi-lab/open-text-books\"] * len(merged_textbook_docs)\n",
    ")\n",
    "\n",
    "# Create HuggingFace Dataset\n",
    "final_dataset = Dataset.from_dict({\n",
    "    \"text\": all_merged_docs,\n",
    "    \"source\": all_sources,\n",
    "})\n",
    "\n",
    "# Optionally, save to disk or push to hub\n",
    "final_dataset.push_to_hub(\"igzi/pile-stem-corpus-small-semantic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp_exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
