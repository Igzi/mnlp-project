{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3c9db7-db32-45a0-81bd-481fa24a660c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394988\n",
      "375\n",
      "\n",
      "#{1,6} Q:\n",
      "501\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "from datasets import load_dataset\n",
    "ds = (\n",
    "            load_dataset(\"igzi/pile-stem-corpus-small\", split=\"train\")\n",
    "            .shuffle(seed=42)\n",
    "            .select(range(min(1000, len(load_dataset(\"igzi/pile-stem-corpus-small\", split=\"train\")))))\n",
    "        )\n",
    "\n",
    "# 1. Load a tokenizer from HuggingFace (any model will do for this test)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"igzi/MNLP_M2_document_encoder\")\n",
    "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=512,  # tokens\n",
    "    chunk_overlap=512//10,  # tokens\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=MARKDOWN_SEPARATORS,\n",
    ")\n",
    "for i in range(10):\n",
    "    # 2. Create a sample markdown-style document\n",
    "    sample_text = ds[i][\"text\"]\n",
    "    \n",
    "    # 3. Wrap the text in a LangChain Document\n",
    "    doc = Document(page_content=sample_text)\n",
    "    \n",
    "    # 6. Perform the splitting\n",
    "    chunks = splitter.split_documents([doc])\n",
    "print(len(sample_text))\n",
    "print(len(chunks))\n",
    "print(sample_text[:10])\n",
    "\n",
    "import re\n",
    "print(len(sample_text.split(MARKDOWN_SEPARATORS[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f22bb2e-5966-4398-b6af-5cdfc6747ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='#{1,6} Q:\n",
      "\n",
      "Proving function has simple pole and residue Suppose $f$ is analytic and not constant on the domain $D \\subseteq \\mathbb{C}$.\n",
      "  \n",
      "  If $z_0 \\in D$ is a zero of $f$ of order $k$, show that the\n",
      "  function $\\frac{f'(z)}{f(z)}$ has a simple pole at $z_0$ with residue\n",
      "  $k$.\n",
      "\n",
      "\n",
      "I am not entirely sure how to manipulate the definition of a residue of orders greater than 1 in order to show this.\n",
      "\n",
      "A:\n",
      "\n",
      "Hint: $f(z) = g(z) (z - z_0)^k$ where $g(z_0) \\ne 0$ is analytic on $D$.  What does this say about $f'(z)/f(z)$?\n",
      "#{1,6} Q:\n",
      "\n",
      "What does it mean: $a\\in X$ is open in $X \\subset \\mathbb{R^n}$ I'm in a course of multivariable real analisys and I have to prove this:\n",
      "\n",
      "\n",
      "  $a\\in X$ is open in $X\\subset\\mathbb{R}^n$ (in the related topology  to $X$) if and only if   $a$ is a isolated point.\n",
      "\n",
      "\n",
      "I don't understand what does  the first part means, after this I can do the proof, please unclear my doubts.\n",
      "\n",
      "A:\n",
      "\n",
      "Here is a more accurate statement: \"For each subset $X \\subset \\mathbb{R}$ and each $a \\in X$, the set $\\{a\\}$ is open (in the topology on $X$ related to $\\mathbb{R}^n$) if and only if $a$ is an isolated point of $X$.\"' metadata={'start_index': 297377}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[283])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3931ade0-6abc-4dc7-b002-85535acb1f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    if len(tokenizer.encode(chunk.page_content, add_special_tokens=False))>512:\n",
    "        print(chunk)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd2df53a-1100-4241-8f08-30b701dcfab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: 67 tokens\n",
      "Chunk 2: 76 tokens\n",
      "Chunk 3: 68 tokens\n",
      "Chunk 4: 119 tokens\n",
      "Chunk 5: 158 tokens\n",
      "Chunk 6: 97 tokens\n",
      "Chunk 7: 201 tokens\n",
      "Chunk 8: 170 tokens\n",
      "Chunk 9: 142 tokens\n",
      "Chunk 10: 65 tokens\n",
      "Chunk 11: 170 tokens\n",
      "Chunk 12: 110 tokens\n",
      "Chunk 13: 82 tokens\n",
      "Chunk 14: 398 tokens\n",
      "Chunk 15: 128 tokens\n",
      "Chunk 16: 241 tokens\n",
      "Chunk 17: 179 tokens\n",
      "Chunk 18: 77 tokens\n",
      "Chunk 19: 63 tokens\n",
      "Chunk 20: 256 tokens\n",
      "Chunk 21: 267 tokens\n",
      "Chunk 22: 115 tokens\n",
      "Chunk 23: 228 tokens\n",
      "Chunk 24: 385 tokens\n",
      "Chunk 25: 117 tokens\n",
      "Chunk 26: 262 tokens\n",
      "Chunk 27: 260 tokens\n",
      "Chunk 28: 85 tokens\n",
      "Chunk 29: 193 tokens\n",
      "Chunk 30: 161 tokens\n",
      "Chunk 31: 137 tokens\n",
      "Chunk 32: 69 tokens\n",
      "Chunk 33: 117 tokens\n",
      "Chunk 34: 329 tokens\n",
      "Chunk 35: 360 tokens\n",
      "Chunk 36: 169 tokens\n",
      "Chunk 37: 54 tokens\n",
      "Chunk 38: 140 tokens\n",
      "Chunk 39: 122 tokens\n",
      "Chunk 40: 133 tokens\n",
      "Chunk 41: 288 tokens\n",
      "Chunk 42: 91 tokens\n",
      "Chunk 43: 142 tokens\n",
      "Chunk 44: 62 tokens\n",
      "Chunk 45: 301 tokens\n",
      "Chunk 46: 98 tokens\n",
      "Chunk 47: 202 tokens\n",
      "Chunk 48: 147 tokens\n",
      "Chunk 49: 51 tokens\n",
      "Chunk 50: 308 tokens\n",
      "Chunk 51: 146 tokens\n",
      "Chunk 52: 52 tokens\n",
      "Chunk 53: 147 tokens\n",
      "Chunk 54: 91 tokens\n",
      "Chunk 55: 62 tokens\n",
      "Chunk 56: 106 tokens\n",
      "Chunk 57: 114 tokens\n",
      "Chunk 58: 348 tokens\n",
      "Chunk 59: 72 tokens\n",
      "Chunk 60: 187 tokens\n",
      "Chunk 61: 201 tokens\n",
      "Chunk 62: 345 tokens\n",
      "Chunk 63: 236 tokens\n",
      "Chunk 64: 199 tokens\n",
      "Chunk 65: 90 tokens\n",
      "Chunk 66: 82 tokens\n",
      "Chunk 67: 58 tokens\n",
      "Chunk 68: 72 tokens\n",
      "Chunk 69: 88 tokens\n",
      "Chunk 70: 400 tokens\n",
      "Chunk 71: 79 tokens\n",
      "Chunk 72: 380 tokens\n",
      "Chunk 73: 65 tokens\n",
      "Chunk 74: 127 tokens\n",
      "Chunk 75: 69 tokens\n",
      "Chunk 76: 303 tokens\n",
      "Chunk 77: 370 tokens\n",
      "Chunk 78: 169 tokens\n",
      "Chunk 79: 97 tokens\n",
      "Chunk 80: 92 tokens\n",
      "Chunk 81: 229 tokens\n",
      "Chunk 82: 193 tokens\n",
      "Chunk 83: 61 tokens\n",
      "Chunk 84: 62 tokens\n",
      "Chunk 85: 169 tokens\n",
      "Chunk 86: 252 tokens\n",
      "Chunk 87: 343 tokens\n",
      "Chunk 88: 98 tokens\n",
      "Chunk 89: 250 tokens\n",
      "Chunk 90: 190 tokens\n",
      "Chunk 91: 63 tokens\n",
      "Chunk 92: 139 tokens\n",
      "Chunk 93: 118 tokens\n",
      "Chunk 94: 143 tokens\n",
      "Chunk 95: 160 tokens\n",
      "Chunk 96: 161 tokens\n",
      "Chunk 97: 65 tokens\n",
      "Chunk 98: 65 tokens\n",
      "Chunk 99: 134 tokens\n",
      "Chunk 100: 53 tokens\n",
      "Chunk 101: 82 tokens\n",
      "Chunk 102: 360 tokens\n",
      "Chunk 103: 190 tokens\n",
      "Chunk 104: 70 tokens\n",
      "Chunk 105: 173 tokens\n",
      "Chunk 106: 172 tokens\n",
      "Chunk 107: 181 tokens\n",
      "Chunk 108: 80 tokens\n",
      "Chunk 109: 115 tokens\n",
      "Chunk 110: 358 tokens\n",
      "Chunk 111: 240 tokens\n",
      "Chunk 112: 236 tokens\n",
      "Chunk 113: 215 tokens\n",
      "Chunk 114: 248 tokens\n",
      "Chunk 115: 109 tokens\n",
      "Chunk 116: 135 tokens\n",
      "Chunk 117: 51 tokens\n",
      "Chunk 118: 218 tokens\n",
      "Chunk 119: 276 tokens\n",
      "Chunk 120: 56 tokens\n",
      "Chunk 121: 50 tokens\n",
      "Chunk 122: 307 tokens\n",
      "Chunk 123: 178 tokens\n",
      "Chunk 124: 104 tokens\n",
      "Chunk 125: 113 tokens\n",
      "Chunk 126: 252 tokens\n",
      "Chunk 127: 113 tokens\n",
      "Chunk 128: 60 tokens\n",
      "Chunk 129: 83 tokens\n",
      "Chunk 130: 59 tokens\n",
      "Chunk 131: 84 tokens\n",
      "Chunk 132: 124 tokens\n",
      "Chunk 133: 125 tokens\n",
      "Chunk 134: 58 tokens\n",
      "Chunk 135: 160 tokens\n",
      "Chunk 136: 109 tokens\n",
      "Chunk 137: 141 tokens\n",
      "Chunk 138: 221 tokens\n",
      "Chunk 139: 58 tokens\n",
      "Chunk 140: 281 tokens\n",
      "Chunk 141: 196 tokens\n",
      "Chunk 142: 201 tokens\n",
      "Chunk 143: 105 tokens\n",
      "Chunk 144: 263 tokens\n",
      "Chunk 145: 259 tokens\n",
      "Chunk 146: 214 tokens\n",
      "Chunk 147: 189 tokens\n",
      "Chunk 148: 249 tokens\n",
      "Chunk 149: 147 tokens\n",
      "Chunk 150: 142 tokens\n",
      "Chunk 151: 160 tokens\n",
      "Chunk 152: 393 tokens\n",
      "Chunk 153: 73 tokens\n",
      "Chunk 154: 77 tokens\n",
      "Chunk 155: 51 tokens\n",
      "Chunk 156: 76 tokens\n",
      "Chunk 157: 55 tokens\n",
      "Chunk 158: 186 tokens\n",
      "Chunk 159: 281 tokens\n",
      "Chunk 160: 70 tokens\n",
      "Chunk 161: 61 tokens\n",
      "Chunk 162: 308 tokens\n",
      "Chunk 163: 295 tokens\n",
      "Chunk 164: 213 tokens\n",
      "Chunk 165: 93 tokens\n",
      "Chunk 166: 90 tokens\n",
      "Chunk 167: 370 tokens\n",
      "Chunk 168: 129 tokens\n",
      "Chunk 169: 138 tokens\n",
      "Chunk 170: 179 tokens\n",
      "Chunk 171: 198 tokens\n",
      "Chunk 172: 229 tokens\n",
      "Chunk 173: 81 tokens\n",
      "Chunk 174: 188 tokens\n",
      "Chunk 175: 84 tokens\n",
      "Chunk 176: 75 tokens\n",
      "Chunk 177: 261 tokens\n",
      "Chunk 178: 92 tokens\n",
      "Chunk 179: 121 tokens\n",
      "Chunk 180: 133 tokens\n",
      "Chunk 181: 109 tokens\n",
      "Chunk 182: 150 tokens\n",
      "Chunk 183: 176 tokens\n",
      "Chunk 184: 189 tokens\n",
      "Chunk 185: 57 tokens\n",
      "Chunk 186: 178 tokens\n",
      "Chunk 187: 69 tokens\n",
      "Chunk 188: 68 tokens\n",
      "Chunk 189: 53 tokens\n",
      "Chunk 190: 90 tokens\n",
      "Chunk 191: 143 tokens\n",
      "Chunk 192: 102 tokens\n",
      "Chunk 193: 86 tokens\n",
      "Chunk 194: 192 tokens\n",
      "Chunk 195: 327 tokens\n",
      "Chunk 196: 166 tokens\n",
      "Chunk 197: 97 tokens\n",
      "Chunk 198: 81 tokens\n",
      "Chunk 199: 83 tokens\n",
      "Chunk 200: 361 tokens\n",
      "\n",
      "Total chunks: 200\n",
      "Average size: 159.01 tokens\n",
      "Max size: 400 tokens\n",
      "Min size: 50 tokens\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"\\n#{1,6} \"\n",
    "\n",
    "# Split the sample text using regex\n",
    "chunks = re.split(pattern, sample_text)\n",
    "\n",
    "# Compute token length of each chunk\n",
    "chunk_sizes = [len(tokenizer.encode(chunk, add_special_tokens=False)) for chunk in chunks]\n",
    "\n",
    "# Print results\n",
    "for i, size in enumerate(chunk_sizes):\n",
    "    print(f\"Chunk {i + 1}: {size} tokens\")\n",
    "\n",
    "# Optionally, summary statistics\n",
    "print(f\"\\nTotal chunks: {len(chunk_sizes)}\")\n",
    "print(f\"Average size: {sum(chunk_sizes)/len(chunk_sizes):.2f} tokens\")\n",
    "print(f\"Max size: {max(chunk_sizes)} tokens\")\n",
    "print(f\"Min size: {min(chunk_sizes)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb28187-0215-4de9-a199-c68e76eed4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNP Project",
   "language": "python",
   "name": "mnlp_m2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
