{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ee9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets, Dataset\n",
    "\n",
    "mmlu_auxiliary_data = []\n",
    "arc_easy_data = []\n",
    "arc_challenge_data = []\n",
    "scienceqa_data = []\n",
    "mathqa_data = []\n",
    "openbookqa_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd09b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of STEM-related MMLU subsets\n",
    "stem_subsets = [\n",
    "    \"abstract_algebra\",\n",
    "    \"anatomy\",\n",
    "    \"astronomy\",\n",
    "    \"college_biology\",\n",
    "    \"college_chemistry\",\n",
    "    \"college_computer_science\",\n",
    "    \"college_mathematics\",\n",
    "    \"college_physics\",\n",
    "    \"computer_security\",\n",
    "    \"conceptual_physics\",\n",
    "    \"electrical_engineering\",\n",
    "    \"elementary_mathematics\",\n",
    "    \"high_school_biology\",\n",
    "    \"high_school_chemistry\",\n",
    "    \"high_school_computer_science\",\n",
    "    \"high_school_mathematics\",\n",
    "    \"high_school_physics\",\n",
    "    \"high_school_statistics\",\n",
    "    \"machine_learning\",\n",
    "]\n",
    "\n",
    "data = load_dataset(\"kz919/mmlu-auxiliary-train-auto-labelled\", split=\"train\")\n",
    "int_to_char_ans = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "cnt = 0\n",
    "for data_point in data:\n",
    "    if data_point[\"task\"] not in stem_subsets:\n",
    "        continue\n",
    "    mmlu_auxiliary_data.append({\n",
    "        \"dataset\": \"kz919/mmlu-auxiliary-train-auto-labelled\",\n",
    "        \"id\": f\"mmlu_auxiliary_train_auto_labelled_{cnt}\",\n",
    "        \"question\": data_point[\"question\"],\n",
    "        \"choices\": data_point[\"choices\"],\n",
    "        \"answer\": int_to_char_ans[data_point[\"answer\"]]\n",
    "    })\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ab0f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"cais/mmlu\", 'all', split=\"validation\")\n",
    "int_to_char_ans = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "cnt = 0\n",
    "mmlu_validation_data = []\n",
    "for data_point in data:\n",
    "    if data_point[\"subject\"] not in stem_subsets:\n",
    "        continue\n",
    "    mmlu_validation_data.append({\n",
    "        \"dataset\": \"cais/mmlu\",\n",
    "        \"id\": f\"mmlu_{cnt}\",\n",
    "        \"question\": data_point[\"question\"],\n",
    "        \"choices\": data_point[\"choices\"],\n",
    "        \"answer\": int_to_char_ans[data_point[\"answer\"]]\n",
    "    })\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d638e3-8c6a-46f7-94e7-9ad77725ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\", split=\"train\")\n",
    "for data_point in data:\n",
    "    if not 'A' <= data_point[\"answerKey\"] <= 'Z':\n",
    "        if data_point[\"answerKey\"] == \"0\":\n",
    "            print(\"ERROR\")\n",
    "        # Convert numeric string to int and then to letter\n",
    "        c = chr(int(data_point[\"answerKey\"]) + ord('A')-1)\n",
    "        data_point[\"answerKey\"] = c\n",
    "\n",
    "    arc_easy_data.append({\n",
    "        \"dataset\": \"allenai/ai2_arc\",\n",
    "        \"id\": f\"arc_easy_{data_point['id']}\",\n",
    "        \"question\": data_point[\"question\"],\n",
    "        \"choices\": data_point[\"choices\"][\"text\"],\n",
    "        \"answer\": data_point[\"answerKey\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea7480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\", split=\"validation\")\n",
    "arc_easy_data_validation = []\n",
    "for data_point in data:\n",
    "    if not 'A' <= data_point[\"answerKey\"] <= 'Z':\n",
    "        if data_point[\"answerKey\"] == \"0\":\n",
    "            print(\"ERROR\")\n",
    "        # Convert numeric string to int and then to letter\n",
    "        c = chr(int(data_point[\"answerKey\"]) + ord('A')-1)\n",
    "        data_point[\"answerKey\"] = c\n",
    "    arc_easy_data_validation.append({\n",
    "        \"dataset\": \"allenai/ai2_arc\",\n",
    "        \"id\": f\"arc_easy_{data_point['id']}\",\n",
    "        \"question\": data_point[\"question\"],\n",
    "        \"choices\": data_point[\"choices\"][\"text\"],\n",
    "        \"answer\": data_point[\"answerKey\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "076a628a-ce91-4345-a222-517323b0f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"train\")\n",
    "for data_point in data:\n",
    "    if not 'A' <= data_point[\"answerKey\"] <= 'Z':\n",
    "        if data_point[\"answerKey\"] == \"0\":\n",
    "            print(\"ERROR\")\n",
    "        # Convert numeric string to int and then to letter\n",
    "        c = chr(int(data_point[\"answerKey\"]) + ord('A')-1)\n",
    "        data_point[\"answerKey\"] = c\n",
    "\n",
    "    arc_challenge_data.append({\n",
    "        \"dataset\": \"allenai/ai2_arc\",\n",
    "        \"id\": f\"arc_challenge_{data_point['id']}\",\n",
    "        \"question\": data_point[\"question\"],\n",
    "        \"choices\": data_point[\"choices\"][\"text\"],\n",
    "        \"answer\": data_point[\"answerKey\"]\n",
    "    })\n",
    "\n",
    "data = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"validation\")\n",
    "arc_challenge_data_validation = []\n",
    "for data_point in data:\n",
    "    if not 'A' <= data_point[\"answerKey\"] <= 'Z':\n",
    "        if data_point[\"answerKey\"] == \"0\":\n",
    "            print(\"ERROR\")\n",
    "        # Convert numeric string to int and then to letter\n",
    "        c = chr(int(data_point[\"answerKey\"]) + ord('A')-1)\n",
    "        data_point[\"answerKey\"] = c\n",
    "    arc_challenge_data_validation.append({\n",
    "        \"dataset\": \"allenai/ai2_arc\",\n",
    "        \"id\": f\"arc_challenge_{data_point['id']}\",\n",
    "        \"question\": data_point[\"question\"],\n",
    "        \"choices\": data_point[\"choices\"][\"text\"],\n",
    "        \"answer\": data_point[\"answerKey\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6647343-23d0-4465-927d-410b07bde0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"derek-thomas/ScienceQA\", split=\"train\")\n",
    "cnt = 0\n",
    "for data_point in data:\n",
    "    if data_point[\"image\"] is not None or data_point[\"subject\"] != \"natural science\" or data_point[\"task\"] != \"closed choice\":\n",
    "        continue\n",
    "    scienceqa_data.append({\n",
    "        \"dataset\": \"derek-thomas/ScienceQA\",\n",
    "        \"id\": f\"scienceqa_{cnt}\",\n",
    "        \"question\": data_point[\"question\"],\n",
    "        \"choices\": data_point[\"choices\"],\n",
    "        \"answer\": int_to_char_ans[data_point[\"answer\"]]\n",
    "    })\n",
    "    cnt += 1\n",
    "\n",
    "data = load_dataset(\"derek-thomas/ScienceQA\", split=\"test\")\n",
    "cnt = 0\n",
    "for data_point in data:\n",
    "    if data_point[\"image\"] is not None or data_point[\"subject\"] != \"natural science\" or data_point[\"task\"] != \"closed choice\":\n",
    "        continue\n",
    "    scienceqa_data.append({\n",
    "        \"dataset\": \"derek-thomas/ScienceQA\",\n",
    "        \"id\": f\"scienceqa_{cnt}\",\n",
    "        \"question\": data_point[\"question\"],\n",
    "        \"choices\": data_point[\"choices\"],\n",
    "        \"answer\": int_to_char_ans[data_point[\"answer\"]]\n",
    "    })\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0998ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"derek-thomas/ScienceQA\", split=\"validation\")\n",
    "cnt = 0\n",
    "scienceqa_data_validation = []\n",
    "for data_point in data:\n",
    "    if data_point[\"image\"] is not None or data_point[\"subject\"] != \"natural science\" or data_point[\"task\"] != \"closed choice\":\n",
    "        continue\n",
    "    scienceqa_data_validation.append({\n",
    "        \"dataset\": \"derek-thomas/ScienceQA\",\n",
    "        \"id\": f\"scienceqa_{cnt}\",\n",
    "        \"question\": data_point[\"question\"],\n",
    "        \"choices\": data_point[\"choices\"],\n",
    "        \"answer\": int_to_char_ans[data_point[\"answer\"]]\n",
    "    })\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1980068-47af-4cc6-993b-27c9a48a6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data = load_dataset(\"allenai/math_qa\", split=\"train\")\n",
    "cnt = 0\n",
    "char_to_char_ans = {\n",
    "    'a': \"A\",\n",
    "    'b': \"B\",\n",
    "    'c': \"C\",\n",
    "    'd': \"D\",\n",
    "    'e': \"E\"\n",
    "}\n",
    "\n",
    "def extract_choices(choices_str):\n",
    "    matches = re.findall(r'[a-e]\\s*\\)\\s*([^,]+)', choices_str)\n",
    "    # Clean up whitespace and dots\n",
    "    res = [m.strip().replace(' .', '.').replace(' ,', ',') for m in matches]\n",
    "    return res\n",
    "\n",
    "for data_point in data:\n",
    "    mathqa_data.append({\n",
    "        \"dataset\": \"allenai/math_qa\",\n",
    "        \"id\": f\"mathqa_{cnt}\",\n",
    "        \"question\": data_point[\"Problem\"],\n",
    "        \"choices\": extract_choices(data_point[\"options\"]),\n",
    "        \"answer\": char_to_char_ans[data_point[\"correct\"]],\n",
    "        \"context\": data_point[\"Rationale\"],\n",
    "    })\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae522f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"allenai/math_qa\", split=\"validation\")\n",
    "cnt = 0\n",
    "\n",
    "mathqa_data_validation = []\n",
    "\n",
    "for data_point in data:\n",
    "    mathqa_data_validation.append({\n",
    "        \"dataset\": \"allenai/math_qa\",\n",
    "        \"id\": f\"mathqa_{cnt}\",\n",
    "        \"question\": data_point[\"Problem\"],\n",
    "        \"choices\": extract_choices(data_point[\"options\"]),\n",
    "        \"answer\": char_to_char_ans[data_point[\"correct\"]],\n",
    "        \"context\": data_point[\"Rationale\"],\n",
    "    })\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4ead71f-9cd6-4ab8-a820-1f3a6faf565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"allenai/openbookqa\", \"additional\", split=\"train\")\n",
    "cnt = 0\n",
    "\n",
    "openbookqa_data = []\n",
    "\n",
    "for data_point in data:\n",
    "    openbookqa_data.append({\n",
    "        \"dataset\": \"allenai/openbookqa\",\n",
    "        \"id\": f\"openbookqa_{cnt}\",\n",
    "        \"question\": data_point[\"question_stem\"],\n",
    "        \"choices\": data_point[\"choices\"][\"text\"],\n",
    "        \"answer\": data_point[\"answerKey\"],\n",
    "        \"context\": data_point[\"fact1\"],\n",
    "    })\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2508cfcf-7b32-40c6-b366-59a0492991d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"allenai/openbookqa\", \"additional\", split=\"validation\")\n",
    "cnt = 0\n",
    "\n",
    "openbookqa_data_validation = []\n",
    "\n",
    "for data_point in data:\n",
    "    openbookqa_data_validation.append({\n",
    "        \"dataset\": \"allenai/openbookqa\",\n",
    "        \"id\": f\"openbookqa_{cnt}\",\n",
    "        \"question\": data_point[\"question_stem\"],\n",
    "        \"choices\": data_point[\"choices\"][\"text\"],\n",
    "        \"answer\": data_point[\"answerKey\"],\n",
    "        \"context\": data_point[\"fact1\"],\n",
    "    })\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa4100ae-40bf-4ac4-890b-eaeb8e88e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data = load_dataset(\"allenai/sciq\", split=\"train\")\n",
    "cnt = 0\n",
    "\n",
    "sciq_data_train = []\n",
    "\n",
    "for data_point in data:\n",
    "    options = [data_point[\"correct_answer\"], data_point[\"distractor1\"], data_point[\"distractor2\"], data_point[\"distractor3\"]]\n",
    "    gold_ix = random.randint(0, 3)\n",
    "    options[0], options[gold_ix] = options[gold_ix], options[0]\n",
    "    sciq_data_train.append({\n",
    "        \"dataset\": \"allenai/sciq\",\n",
    "        \"id\": f\"sciq_{cnt}\",\n",
    "        \"question\": data_point[\"question\"],\n",
    "        \"choices\": options,\n",
    "        \"answer\": gold_ix,\n",
    "        \"context\": data_point[\"support\"],\n",
    "    })\n",
    "    cnt += 1\n",
    "\n",
    "data = load_dataset(\"allenai/sciq\", split=\"validation\")\n",
    "sciq_data_validation = []\n",
    "cnt = 0\n",
    "\n",
    "for data_point in data:\n",
    "    options = [data_point[\"correct_answer\"], data_point[\"distractor1\"], data_point[\"distractor2\"], data_point[\"distractor3\"]]\n",
    "    gold_ix = random.randint(0, 3)\n",
    "    options[0], options[gold_ix] = options[gold_ix], options[0]\n",
    "    sciq_data_validation.append({\n",
    "        \"dataset\": \"allenai/sciq\",\n",
    "        \"id\": f\"sciq_{cnt}\",\n",
    "        \"question\": data_point[\"question\"],\n",
    "        \"choices\": options,\n",
    "        \"answer\": gold_ix,\n",
    "        \"context\": data_point[\"support\"],\n",
    "    })\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c5082c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_hf(subset_name, train_dataset, validation_dataset=None):\n",
    "    train_dataset = Dataset.from_list(train_dataset)\n",
    "    if validation_dataset is not None:\n",
    "        validation_dataset = Dataset.from_list(validation_dataset)\n",
    "        dataset_dict = DatasetDict({\n",
    "            \"train\": train_dataset,\n",
    "            \"validation\": validation_dataset\n",
    "        })\n",
    "    else:\n",
    "        dataset_dict = DatasetDict({\n",
    "            \"train\": train_dataset\n",
    "        })\n",
    "    dataset_dict.push_to_hub(\"igzi/MNLP_M2_mcqa_dataset\", config_name=subset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eed722b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eba241b81a044d0a3f164084a4551a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5daea5f8b53a4eb8bd068a40d8b0f403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aba678b7c6a42dc964c3d057738da23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1292c0318a4a47934afa4aec558099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4baf056e213a46ef936c9836872f4f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5794c205b9b4a84919d99a2e6f5088a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136a8e81c7834dc99fd66a7bf362038e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fc54af037a4ed9af01656748fc9aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039fbd446614416db1cc0df3daefd96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0df7acd7ee4093b039d48a94c26f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f093ae2239f46d6b721b26edc1117a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f09f83aca49436fbc42d51a992e3bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f7d4d16a66406ca27e644c54bee21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75640b5dc2d84ec6927c4b4fd0ca64eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2582d8673c4adba09a381784bae212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc95f1fdd76442d7884edc678bbb4785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3177e8a50342ae8d35bb430bd99067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba060a7a55b4f3a87269a29d231539e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48283eb161fb45c38cce2490d63490c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161a66e7bbde4c5da6ffd806192d2a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All datasets in `DatasetDict` should have the same features but features for 'train' and 'validation' don't match: {'dataset': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'choices': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'answer': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None)} != {}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m push_to_hf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScienceQA\u001b[39m\u001b[38;5;124m\"\u001b[39m, scienceqa_data, scienceqa_data_validation)\n\u001b[1;32m      5\u001b[0m push_to_hf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMathQA\u001b[39m\u001b[38;5;124m\"\u001b[39m, mathqa_data, mathqa_data_validation)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mpush_to_hf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOpenBookQA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenbookqa_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenbookqa_data_validation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m push_to_hf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSciQ\u001b[39m\u001b[38;5;124m\"\u001b[39m, sciq_data_train, sciq_data_validation)\n",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m, in \u001b[0;36mpush_to_hf\u001b[0;34m(subset_name, train_dataset, validation_dataset)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     dataset_dict \u001b[38;5;241m=\u001b[39m DatasetDict({\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_dataset\n\u001b[1;32m     12\u001b[0m     })\n\u001b[0;32m---> 13\u001b[0m \u001b[43mdataset_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43migzi/MNLP_M2_mcqa_dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/datasets/dataset_dict.py:1632\u001b[0m, in \u001b[0;36mDatasetDict.push_to_hub\u001b[0;34m(self, repo_id, config_name, set_default, data_dir, commit_message, commit_description, private, token, revision, create_pr, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   1627\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1628\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide one `num_shards` per dataset in the dataset dictionary, e.g. \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: 128, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: 4}}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1629\u001b[0m     )\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m-> 1632\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_values_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1633\u001b[0m total_uploaded_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1634\u001b[0m total_dataset_nbytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/datasets/dataset_dict.py:55\u001b[0m, in \u001b[0;36mDatasetDict._check_values_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item_a, item_b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(items[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], items[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item_a[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m!=\u001b[39m item_b[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[0;32m---> 55\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll datasets in `DatasetDict` should have the same features but features for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_a[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_b[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_a[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_b[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: All datasets in `DatasetDict` should have the same features but features for 'train' and 'validation' don't match: {'dataset': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'choices': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'answer': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None)} != {}"
     ]
    }
   ],
   "source": [
    "push_to_hf(\"MMLU\", mmlu_auxiliary_data, mmlu_validation_data)\n",
    "push_to_hf(\"ARC-Easy\", arc_easy_data, arc_easy_data_validation)\n",
    "push_to_hf(\"ARC-Challenge\", arc_challenge_data, arc_challenge_data_validation)\n",
    "push_to_hf(\"ScienceQA\", scienceqa_data, scienceqa_data_validation)\n",
    "push_to_hf(\"MathQA\", mathqa_data, mathqa_data_validation)\n",
    "push_to_hf(\"OpenBookQA\", openbookqa_data, openbookqa_data_validation)\n",
    "push_to_hf(\"SciQ\", sciq_data_train, sciq_data_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
