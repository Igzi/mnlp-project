{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07715a05-580d-4ea0-a72f-344f26ba557a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae58c9e-66c1-479c-bef8-22981a2cac7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7660a0ac40054a13bb64696c150e8e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n",
      "/home/my_venvs/mnlp_m2/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/my_venvs/mnlp_m2/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/my_venvs/mnlp_m2/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"Qwen/Qwen3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\n",
    "\n",
    "# Text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Prompt template\n",
    "def generate_prompt(document_text):\n",
    "    return f\"\"\"Based on the following passage, generate one multiple-choice question with exactly 4 answer options (Aâ€“D). Indicate the correct answer clearly.\n",
    "\n",
    "Passage:\n",
    "\\\"\\\"\\\"{document_text}\\\"\\\"\\\"\n",
    "\n",
    "Question:\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate MCQs\n",
    "def generate_mcqs(documents, max_tokens=512):\n",
    "    questions = []\n",
    "    for doc in documents:\n",
    "        prompt = generate_prompt(doc)\n",
    "        output = generator(prompt, max_new_tokens=max_tokens, do_sample=False)[0][\"generated_text\"]\n",
    "        question = output[len(prompt):].strip()\n",
    "        questions.append(question)\n",
    "    return questions\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"Photosynthesis is the process by which green plants use sunlight to synthesize foods from carbon dioxide and water.\",\n",
    "    \"The capital of France is Paris, which is known for its art, fashion, and the iconic Eiffel Tower.\",\n",
    "]\n",
    "\n",
    "# Generate MCQs\n",
    "mcqs = generate_mcqs(documents)\n",
    "\n",
    "# Print the output\n",
    "for i, mcq in enumerate(mcqs, 1):\n",
    "    print(f\"MCQ #{i}:\\n{mcq}\\n{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed60e557-713e-40fe-bbc0-cb5d451fb68c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLNP Project",
   "language": "python",
   "name": "mnlp_m2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
